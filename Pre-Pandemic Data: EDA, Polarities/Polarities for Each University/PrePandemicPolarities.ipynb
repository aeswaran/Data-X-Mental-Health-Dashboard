{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /opt/conda/lib/python3.8/site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in /opt/conda/lib/python3.8/site-packages (from textblob) (3.5)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk>=3.1->textblob) (1.0.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.8/site-packages (from nltk>=3.1->textblob) (2021.4.4)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from nltk>=3.1->textblob) (4.56.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /opt/conda/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /opt/conda/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /opt/conda/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /opt/conda/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import Beautiful Soup, NumPy and Pandas, etc\n",
    "import bs4 as bs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import hashlib\n",
    " \n",
    "# download NLTK classifiers - these are cached locally on your machine\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# import ml classifiers\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "from nltk.stem import PorterStemmer     # parsing/stemmer\n",
    "from nltk.tag import pos_tag            # parts-of-speech tagging\n",
    "from nltk.corpus import wordnet         # sentiment scores\n",
    "from nltk.stem import WordNetLemmatizer # stem and context\n",
    "from nltk.corpus import stopwords       # stopwords\n",
    "from nltk.util import ngrams            # ngram iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "## NLP Code to clean texts, and lemmatize prior to finding polarity scores\n",
    "## taken from HW3 Solution Video\n",
    "def reviewcleaner(review, lemmatize = True, stem = False):\n",
    "    if lemmatize == True and stem == True:\n",
    "        raise RuntimeError(\"May not pass both as true\")\n",
    "    #Remove HTML Tags\n",
    "    review = bs.BeautifulSoup(review).text\n",
    "    \n",
    "    #use regex to find emoticons\n",
    "    emoticons = re.findall(' (?::|;|=)(?:-)?(?:\\)|\\(|D|P)',review)\n",
    "    \n",
    "    #Remove punctuation\n",
    "    review = re.sub9(\"[^a-zA-Z]\", ' ', review)\n",
    "    \n",
    "    #Tokenize into words (all lower case)\n",
    "    review = review.lower().split()\n",
    "    \n",
    "    #remove stopwords, lemmatize, stem\n",
    "    clean_review = []\n",
    "    for word in review: \n",
    "        if word not in eng_stopwords:\n",
    "            if lemmatize is True:\n",
    "                word = wnl.lemmatize(word)\n",
    "            elif stem is True:\n",
    "                word = ps.stem(word)\n",
    "            clean_review.append(word)\n",
    "            \n",
    "    #join the review to one sentence\n",
    "    review_processed = ' '.join(clean_review + emoticons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to deal with missing values in text of social media posts\n",
    "def fillnaf (tbl):\n",
    "    tbl1 = tbl[\"Body\"].fillna(tbl[\"Title\"])\n",
    "    tbl[\"Body\"] = tbl1\n",
    "    return tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Polarities per University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in Berkeley Data (pre-pandemic)\n",
    "berk = pd.read_csv('berkprepandemic.csv')\n",
    "berk = fillnaf(berk)\n",
    "#berk = review_cleaner(berk['Body'], True, False)\n",
    "\n",
    "#Calculates polarity scores for each post\n",
    "polscores = []\n",
    "subscores = []\n",
    "for i in range(0,6760):\n",
    "    score = TextBlob(berk[\"Body\"][i]).polarity\n",
    "    sub = TextBlob(berk['Body'][i]).subjectivity\n",
    "    polscores.append(score)\n",
    "    subscores.append(sub)\n",
    "berk[\"Polarity\"] = polscores\n",
    "berk['Subjectivity'] = subscores\n",
    "berk['University'] = 'UC Berkeley'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in UCLA Data (pre-pandemic)\n",
    "ucla = pd.read_csv('uclaprepandemic.csv')\n",
    "ucla = fillnaf(ucla)\n",
    "#ucla = review_cleaner(ucla['Body'], True, False)\n",
    "\n",
    "#Calculates polarity scores for each post\n",
    "polscores = []\n",
    "subscores = []\n",
    "for i in range(0,7892):\n",
    "    score = TextBlob(ucla[\"Body\"][i]).polarity\n",
    "    sub = TextBlob(ucla['Body'][i]).subjectivity\n",
    "    polscores.append(score)\n",
    "    subscores.append(sub)\n",
    "ucla[\"Polarity\"] = polscores\n",
    "ucla['Subjectivity'] = subscores\n",
    "ucla['University'] = 'UCLA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in Cornell Data (pre-pandemic)\n",
    "cornell = pd.read_csv('cornellprepandemic.csv')\n",
    "cornell = fillnaf(cornell)\n",
    "#cornell = review_cleaner(cornell['Body'], True, False)\n",
    "\n",
    "#Calculates polarity scores for each post\n",
    "polscores = []\n",
    "subscores = []\n",
    "for i in range(0,5973):\n",
    "    score = TextBlob(cornell[\"Body\"][i]).polarity\n",
    "    sub = TextBlob(cornell['Body'][i]).subjectivity\n",
    "    polscores.append(score)\n",
    "    subscores.append(sub)\n",
    "cornell[\"Polarity\"] = polscores\n",
    "cornell['Subjectivity'] = subscores\n",
    "cornell['University'] = 'Cornell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in UC San Diego Data (pre-pandemic)\n",
    "ucsd = pd.read_csv('ucsdprepandemic.csv')\n",
    "ucsd = fillnaf(ucsd)\n",
    "#ucsd = review_cleaner(ucsd['Body'], True, False)\n",
    "\n",
    "#Calculates polarity scores for each post\n",
    "polscores = []\n",
    "subscores = []\n",
    "for i in range(0,7843):\n",
    "    score = TextBlob(ucsd[\"Body\"][i]).polarity\n",
    "    sub = TextBlob(ucsd['Body'][i]).subjectivity\n",
    "    polscores.append(score)\n",
    "    subscores.append(sub)\n",
    "ucsd[\"Polarity\"] = polscores\n",
    "ucsd['Subjectivity'] = subscores\n",
    "ucsd['University'] = 'UC San Diego'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in UC Santa Barbara Data (pre-pandemic)\n",
    "ucsb = pd.read_csv('ucsbprepandemic.csv')\n",
    "ucsb = fillnaf(ucsb)\n",
    "#ucsb = review_cleaner(ucsd['Body'], True, False)\n",
    "\n",
    "#Calculates polarity scores for each post\n",
    "polscores = []\n",
    "subscores = []\n",
    "for i in range(0,6007):\n",
    "    score = TextBlob(ucsb[\"Body\"][i]).polarity\n",
    "    sub = TextBlob(ucsb['Body'][i]).subjectivity\n",
    "    polscores.append(score)\n",
    "    subscores.append(sub)\n",
    "ucsb[\"Polarity\"] = polscores\n",
    "ucsb['Subjectivity'] = subscores\n",
    "ucsb['University'] = 'UC Santa Barbara'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in UC Irvine Data (pre-pandemic)\n",
    "irvine = pd.read_csv('irvineprepandemic.csv')\n",
    "irvine = fillnaf(irvine)\n",
    "#irvine = review_cleaner(irvine['Body'], True, False)\n",
    "\n",
    "#Calculates polarity scores for each post\n",
    "polscores = []\n",
    "subscores = []\n",
    "for i in range(0,7588):\n",
    "    score = TextBlob(irvine[\"Body\"][i]).polarity\n",
    "    sub = TextBlob(irvine['Body'][i]).subjectivity\n",
    "    polscores.append(score)\n",
    "    subscores.append(sub)\n",
    "irvine[\"Polarity\"] = polscores\n",
    "irvine['Subjectivity'] = subscores\n",
    "irvine['University'] = 'UC Irvine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in UC Santa Cruz Data (pre-pandemic)\n",
    "ucsc = pd.read_csv('ucscprepandemic.csv')\n",
    "ucsc = fillnaf(ucsc)\n",
    "#ucsc = review_cleaner(ucsc['Body'], True, False)\n",
    "\n",
    "#Calculates polarity scores for each post\n",
    "polscores = []\n",
    "subscores = []\n",
    "for i in range(0,8722):\n",
    "    score = TextBlob(ucsc[\"Body\"][i]).polarity\n",
    "    sub = TextBlob(ucsc['Body'][i]).subjectivity\n",
    "    polscores.append(score)\n",
    "    subscores.append(sub)\n",
    "ucsc[\"Polarity\"] = polscores\n",
    "ucsc['Subjectivity'] = subscores\n",
    "ucsc['University'] = 'UC Santa Cruz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in UC Davis Data (pre-pandemic)\n",
    "davis = pd.read_csv('ucdprepandemic.csv')\n",
    "davis = fillnaf(davis)\n",
    "#davis = review_cleaner(davis['Body'], True, False)\n",
    "\n",
    "#Calculates polarity scores for each post\n",
    "polscores = []\n",
    "subscores = []\n",
    "for i in range(0,4528):\n",
    "    score = TextBlob(davis[\"Body\"][i]).polarity\n",
    "    sub = TextBlob(davis['Body'][i]).subjectivity\n",
    "    polscores.append(score)\n",
    "    subscores.append(sub)\n",
    "davis[\"Polarity\"] = polscores\n",
    "davis['Subjectivity'] = subscores\n",
    "davis['University'] = 'UC Davis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in NYU Data (pre-pandemic)\n",
    "nyu = pd.read_csv('nyuprepandemic.csv')\n",
    "nyu = fillnaf(nyu)\n",
    "#nyu = review_cleaner(nyu['Body'], True, False)\n",
    "\n",
    "#Calculates polarity scores for each post\n",
    "polscores = []\n",
    "subscores = []\n",
    "for i in range(0,2168):\n",
    "    score = TextBlob(nyu[\"Body\"][i]).polarity\n",
    "    sub = TextBlob(nyu['Body'][i]).subjectivity\n",
    "    polscores.append(score)\n",
    "    subscores.append(sub)\n",
    "nyu[\"Polarity\"] = polscores\n",
    "nyu['Subjectivity'] = subscores\n",
    "nyu['University'] = 'NYU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in UC Riverside Data (pre-pandemic)\n",
    "riverside = pd.read_csv('riversideprepandemic.csv')\n",
    "riverside = fillnaf(riverside)\n",
    "#riverside = review_cleaner(riverside['Body'], True, False)\n",
    "\n",
    "#Calculates polarity scores for each post\n",
    "polscores = []\n",
    "subscores = []\n",
    "for i in range(0,3694):\n",
    "    score = TextBlob(riverside[\"Body\"][i]).polarity\n",
    "    sub = TextBlob(riverside['Body'][i]).subjectivity\n",
    "    polscores.append(score)\n",
    "    subscores.append(sub)\n",
    "riverside[\"Polarity\"] = polscores\n",
    "riverside['Subjectivity'] = subscores\n",
    "riverside['University'] = 'UC Riverside'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in Stanford Data (pre-pandemic)\n",
    "stanford = pd.read_csv('stanfordprepandemic.csv')\n",
    "stanford = fillnaf(stanford)\n",
    "#stanford = review_cleaner(stanford['Body'], True, False)\n",
    "\n",
    "#Calculates polarity scores for each post\n",
    "polscores = []\n",
    "subscores = []\n",
    "for i in range(0,1258):\n",
    "    score = TextBlob(stanford[\"Body\"][i]).polarity\n",
    "    sub = TextBlob(stanford['Body'][i]).subjectivity\n",
    "    polscores.append(score)\n",
    "    subscores.append(sub)\n",
    "stanford[\"Polarity\"] = polscores\n",
    "stanford['Subjectivity'] = subscores\n",
    "stanford['University'] = 'Stanford'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in Harvard Data (pre-pandemic)\n",
    "harvard = pd.read_csv('harvardprepandemic.csv')\n",
    "harvard = fillnaf(harvard)\n",
    "#harvard = review_cleaner(harvard['Body'], True, False)\n",
    "\n",
    "#Calculates polarity scores for each post\n",
    "polscores = []\n",
    "subscores = []\n",
    "for i in range(0,827):\n",
    "    score = TextBlob(harvard[\"Body\"][i]).polarity\n",
    "    sub = TextBlob(harvard['Body'][i]).subjectivity\n",
    "    polscores.append(score)\n",
    "    subscores.append(sub)\n",
    "harvard[\"Polarity\"] = polscores\n",
    "harvard['Subjectivity'] = subscores\n",
    "harvard['University'] = 'Harvard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in MIT Data (pre-pandemic)\n",
    "mit = pd.read_csv('mitprepandemic.csv')\n",
    "mit = fillnaf(mit)\n",
    "#mit = review_cleaner(mit['Body'], True, False)\n",
    "\n",
    "#Calculates polarity scores for each post\n",
    "polscores = []\n",
    "subscores = []\n",
    "for i in range(0,572):\n",
    "    score = TextBlob(mit[\"Body\"][i]).polarity\n",
    "    sub = TextBlob(mit['Body'][i]).subjectivity\n",
    "    polscores.append(score)\n",
    "    subscores.append(sub)\n",
    "mit[\"Polarity\"] = polscores\n",
    "mit['Subjectivity'] = subscores\n",
    "mit['University'] = 'MIT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in UC Merced Data (pre-pandemic)\n",
    "merced = pd.read_csv('ucmprepandemic.csv')\n",
    "merced = fillnaf(merced)\n",
    "#merced = review_cleaner(merced['Body'], True, False)\n",
    "\n",
    "#Calculates polarity scores for each post\n",
    "polscores = []\n",
    "subscores = []\n",
    "for i in range(0,100):\n",
    "    score = TextBlob(merced[\"Body\"][i]).polarity\n",
    "    sub = TextBlob(merced['Body'][i]).subjectivity\n",
    "    polscores.append(score)\n",
    "    subscores.append(sub)\n",
    "merced[\"Polarity\"] = polscores\n",
    "merced['Subjectivity'] = subscores\n",
    "merced['University'] = 'UC Merced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in Yale Data (pre-pandemic)\n",
    "yale = pd.read_csv('yaleprepandemic.csv')\n",
    "yale = fillnaf(yale)\n",
    "#yale = review_cleaner(yale['Body'], True, False)\n",
    "\n",
    "#Calculates polarity scores for each post\n",
    "polscores = []\n",
    "subscores = []\n",
    "for i in range(0,379):\n",
    "    score = TextBlob(yale[\"Body\"][i]).polarity\n",
    "    sub = TextBlob(yale['Body'][i]).subjectivity\n",
    "    polscores.append(score)\n",
    "    subscores.append(sub)\n",
    "yale[\"Polarity\"] = polscores\n",
    "yale['Subjectivity'] = subscores\n",
    "yale['University'] = 'Yale'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in Princeton Data (pre-pandemic)\n",
    "princeton = pd.read_csv('princetonprepandemic.csv')\n",
    "princeton = fillnaf(princeton)\n",
    "#princeton = review_cleaner(princeton['Body'], True, False)\n",
    "\n",
    "#Calculates polarity scores for each post\n",
    "polscores = []\n",
    "subscores = []\n",
    "for i in range(0,213):\n",
    "    score = TextBlob(princeton[\"Body\"][i]).polarity\n",
    "    sub = TextBlob(princeton['Body'][i]).subjectivity\n",
    "    polscores.append(score)\n",
    "    subscores.append(sub)\n",
    "princeton[\"Polarity\"] = polscores\n",
    "princeton['Subjectivity'] = subscores\n",
    "princeton['University'] = 'Princeton'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining All Frames to One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining all dataframes to one\n",
    "frames = [berk, ucla, ucsd, merced, ucsc, ucsb, riverside, davis, irvine, stanford, mit, yale, princeton, cornell, harvard, nyu]\n",
    "alluniversitypolarity = pd.concat(frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Unnecessary Columns and Convert Dataframe to Csv\n",
    "alluniversitypolarity.drop(columns=['Post ID', 'Url', 'Author','Permalink','Flair'], inplace = True)\n",
    "alluniversitypolarity.drop(columns = ['Title','Score'], inplace=True)\n",
    "alluniversitypolarity.drop(columns = ['Body'], inplace=True)\n",
    "alluniversitypolarity.to_csv('prepandemicpolarity.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
