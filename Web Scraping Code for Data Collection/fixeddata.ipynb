{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fixeddata.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyarnetivJPG"
      },
      "source": [
        "# Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfmIce345UaB"
      },
      "source": [
        "import pandas as pd\n",
        "import requests #Pushshift accesses Reddit via an url so this is needed\n",
        "import json #JSON manipulation\n",
        "import csv #To Convert final table into a csv file to save to your machine\n",
        "import time\n",
        "import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFPXbeD-0kGc"
      },
      "source": [
        "# Parameters for your Pushshift URL\n",
        "These are probably the most important parameters to consider when building your Pushshift URL:\n",
        "\n",
        "* size — increase limit of returned entries to 1000\n",
        "* after — where to start the search\n",
        "* before — where to end the search\n",
        "* title — to search only within the submission’s title\n",
        "* subreddit — to narrow it down to a particular subreddit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDma-H_k0frf"
      },
      "source": [
        "#Adapted from this https://gist.github.com/dylankilkenny/3dbf6123527260165f8c5c3bc3ee331b\n",
        "#This function builds an Pushshift URL, accesses the webpage and stores JSON data in a nested list\n",
        "def getPushshiftData(after, before, sub):\n",
        "    #Build URL\n",
        "    url = 'https://api.pushshift.io/reddit/search/submission/?'+'size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
        "    #Print URL to show user\n",
        "    print(url)\n",
        "    #Request URL\n",
        "    r = requests.get(url)\n",
        "    #Load JSON data from webpage into data variable\n",
        "    data = json.loads(r.text)\n",
        "    #return the data element which contains all the submissions data\n",
        "    return data['data']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H524E6kkvh1O"
      },
      "source": [
        "# Extract key information from Submissions\n",
        "\n",
        "We want key data for further analysis including: \n",
        "* Submission Title\n",
        "* URL \n",
        "* Flair\n",
        "* Author\n",
        "* Submission post ID\n",
        "* Score\n",
        "* Upload Time\n",
        "* No. of Comments \n",
        "* Permalink.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6qX7glQ1-nl"
      },
      "source": [
        "#This function will be used to extract the key data points from each JSON result\n",
        "def collectSubData(subm):\n",
        "    #subData was created at the start to hold all the data which is then added to our global subStats dictionary.\n",
        "    subData = list() #list to store data points\n",
        "    title = subm['title']\n",
        "    url = subm['url']\n",
        "    try:\n",
        "        body = subm['selftext']\n",
        "    except KeyError:\n",
        "        body = \"NaN\" \n",
        "    #flairs are not always present so we wrap in try/except\n",
        "    try:\n",
        "        flair = subm['link_flair_text']\n",
        "    except KeyError:\n",
        "        flair = \"NaN\"    \n",
        "    author = subm['author']\n",
        "    sub_id = subm['id']\n",
        "    score = subm['score']\n",
        "    created = datetime.datetime.fromtimestamp(subm['created_utc']) #1520561700.0\n",
        "    numComms = subm['num_comments']\n",
        "    permalink = subm['permalink']\n",
        "\n",
        "    #Put all data points into a tuple and append to subData\n",
        "    subData.append((sub_id,title,url,author,score,created,numComms,permalink,flair,body))\n",
        "    #Create a dictionary entry of current submission data and store all data related to it\n",
        "    subStats[sub_id] = subData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCDRrn9nwRsj"
      },
      "source": [
        "# Update your Search Settings here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-raGl72n6JAD"
      },
      "source": [
        "only need to change after and before"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0fNU7eS2mwT"
      },
      "source": [
        "#Create your timestamps and queries for your search URL\n",
        "#https://www.unixtimestamp.com/index.php > Use this to create your timestamps\n",
        "after = \"1609459200\" #Submissions after this timestamp (1577836800 = 03/01  20)\n",
        "before = \"1612137600\" #Submissions before this timestamp (Sun 04 01 2020 00:00:00 GMT-0800 (Pacific Standard Time))\n",
        "#1606784400\n",
        "#query = \"Cyberpunk 2077\" #Keyword(s) to look for in submissions\n",
        "sub = \"berkeley\" #Which Subreddit to search in\n",
        "\n",
        "#subCount tracks the no. of total submissions we collect\n",
        "subCount = 0\n",
        "#subStats is the dictionary where we will store our data.\n",
        "subStats = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wP_j8pp2-M8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c310347-7011-419e-90a7-b7ecbc365c85"
      },
      "source": [
        "# We need to run this function outside the loop first to get the updated after variable\n",
        "data = getPushshiftData(after, before, sub)\n",
        "# Will run until all posts have been gathered i.e. When the length of data variable = 0\n",
        "# from the 'after' date up until before date\n",
        "while len(data) > 0: #The length of data is the number submissions (data[0], data[1] etc), once it hits zero (after and before vars are the same) end\n",
        "    for submission in data:\n",
        "        collectSubData(submission)\n",
        "        subCount+=1\n",
        "    # Calls getPushshiftData() with the created date of the last submission\n",
        "    print(len(data))\n",
        "    print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
        "    #update after variable to last created date of submission\n",
        "    after = data[-1]['created_utc']\n",
        "    #data has changed due to the new after variable provided by above code\n",
        "    data = getPushshiftData(after, before, sub)\n",
        "    \n",
        "print(len(data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1609459200&before=1612137600&subreddit=berkeley\n",
            "100\n",
            "2021-01-04 00:18:02\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1609719482&before=1612137600&subreddit=berkeley\n",
            "100\n",
            "2021-01-05 22:01:28\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1609884088&before=1612137600&subreddit=berkeley\n",
            "100\n",
            "2021-01-07 20:12:41\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1610050361&before=1612137600&subreddit=berkeley\n",
            "100\n",
            "2021-01-10 01:31:28\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1610242288&before=1612137600&subreddit=berkeley\n",
            "100\n",
            "2021-01-11 23:01:38\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1610406098&before=1612137600&subreddit=berkeley\n",
            "100\n",
            "2021-01-13 07:25:18\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1610522718&before=1612137600&subreddit=berkeley\n",
            "100\n",
            "2021-01-15 08:14:44\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1610698484&before=1612137600&subreddit=berkeley\n",
            "100\n",
            "2021-01-17 04:04:37\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1610856277&before=1612137600&subreddit=berkeley\n",
            "100\n",
            "2021-01-18 18:49:35\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1610995775&before=1612137600&subreddit=berkeley\n",
            "100\n",
            "2021-01-19 18:51:58\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1611082318&before=1612137600&subreddit=berkeley\n",
            "100\n",
            "2021-01-20 18:33:41\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1611167621&before=1612137600&subreddit=berkeley\n",
            "100\n",
            "2021-01-22 02:37:20\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1611283040&before=1612137600&subreddit=berkeley\n",
            "100\n",
            "2021-01-23 23:22:58\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1611444178&before=1612137600&subreddit=berkeley\n",
            "100\n",
            "2021-01-25 20:22:24\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1611606144&before=1612137600&subreddit=berkeley\n",
            "100\n",
            "2021-01-28 02:37:20\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1611801440&before=1612137600&subreddit=berkeley\n",
            "100\n",
            "2021-01-29 23:29:33\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1611962973&before=1612137600&subreddit=berkeley\n",
            "100\n",
            "2021-01-31 11:42:06\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1612093326&before=1612137600&subreddit=berkeley\n",
            "22\n",
            "2021-01-31 23:57:33\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1612137453&before=1612137600&subreddit=berkeley\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaeTGu7mwyoU"
      },
      "source": [
        "# Check your Submission Extraction was successful"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVLuSJ8e8p7i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35a3869e-4b9e-48e4-cabf-a4f6e399fab6"
      },
      "source": [
        "print(str(len(subStats)) + \" submissions have added to list\")\n",
        "print(\"1st entry is:\")\n",
        "print(list(subStats.values())[0][0][1] + \" created: \" + str(list(subStats.values())[0][0][5]))\n",
        "print(\"Last entry is:\")\n",
        "print(list(subStats.values())[-1][0][1] + \" created: \" + str(list(subStats.values())[-1][0][5]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1722 submissions have added to list\n",
            "1st entry is:\n",
            "Haas Applicant grade submission created: 2021-01-01 02:17:50\n",
            "Last entry is:\n",
            "Discipline for gathering created: 2021-01-31 23:57:33\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAm_zZZfw521"
      },
      "source": [
        "# Save data to CSV file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBikywNJ8ufl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f567c22-fa6c-4041-e53b-9b12c37dc4c5"
      },
      "source": [
        "def updateSubs_file():\n",
        "    upload_count = 0\n",
        "    #location = \"\\\\Reddit Data\\\\\" >> If you're running this outside of a notebook you'll need this to direct to a specific location\n",
        "    print(\"input filename of submission file, please add .csv\")\n",
        "    filename = input() #This asks the user what to name the file\n",
        "    file = filename\n",
        "    with open(file, 'w', newline='', encoding='utf-8') as file: \n",
        "        a = csv.writer(file, delimiter=',')\n",
        "        headers = [\"Post ID\",\"Title\",\"Url\",\"Author\",\"Score\",\"Publish Date\",\"Total No. of Comments\",\"Permalink\",\"Flair\",\"Body\"]\n",
        "        a.writerow(headers)\n",
        "        for sub in subStats:\n",
        "            a.writerow(subStats[sub][0])\n",
        "            upload_count+=1\n",
        "            \n",
        "        print(str(upload_count) + \" submissions have been uploaded\")\n",
        "updateSubs_file()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input filename of submission file, please add .csv\n",
            "Jan2021.csv\n",
            "1722 submissions have been uploaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYrpmueXzVxp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}